{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Representation\n",
    "In this section we review the structure of neural networks and the individual units which make up those networks. \n",
    "\n",
    "## Preliminaries \n",
    "We begin by reintroducing some functions from our Logistic Regression and Linear Regression notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def standardize(x):\n",
    "    col_means = np.mean(x,axis=0)\n",
    "    col_std = np.std(x,axis=0)\n",
    "    return (x - col_means) / col_std , col_means, col_std\n",
    "\n",
    "def h_linear(theta,X):\n",
    "    z = np.matmul(theta,X)\n",
    "    return z\n",
    "\n",
    "def h_logistic(theta,X):\n",
    "    z = np.matmul(theta,X)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def cost_logistic(theta,xi,yi):\n",
    "    # note - this function calculate the cost for all variations of theta. A vector is returned. \n",
    "    z = np.matmul(theta,xi)\n",
    "    h = 1 / (1 + np.exp(-z))\n",
    "    if yi == 1:\n",
    "        return -np.log(h)\n",
    "    elif yi == 0:\n",
    "        return -np.log(1-h)\n",
    "    else: raise ValueError('yi was neither 0 nor 1: ' + str(yi))\n",
    "        \n",
    "def cost_linear(theta,xi,yi):\n",
    "    # note - this function calculate the cost for all variations of theta. A vector is returned. \n",
    "    z = np.matmul(theta,xi)\n",
    "    h = 1 / (1 + np.exp(-z))\n",
    "    rv = ((h - yi)**2) / 2\n",
    "    return rv  \n",
    "\n",
    "def J(theta,x,y,f): \n",
    "    s = 0\n",
    "    m = len(x)\n",
    "    for i in range(0,m):\n",
    "        v = f(theta,x[i],y[i])\n",
    "        s+= v\n",
    "    return s / m\n",
    "\n",
    "def gd(theta_in,min_delta_in,a_in,X_in,Y_in,h,maxiterations=-1,lambda_in=0,cost=cost_logistic):\n",
    "    m = len(Y_in) # number of training examples\n",
    "    _ , n = np.shape(X_in) # number of features\n",
    "    \n",
    "    # define a list used to store the costs - these do not have to be calculated but are useful for debugging\n",
    "    c = []\n",
    "    \n",
    "    # Create a theta variable which we will use to keep track of the current value of theta\n",
    "    theta = theta_in\n",
    "    \n",
    "    # Create a list which we will use for storing theta results during optimization\n",
    "    thetas = np.array(theta_in,ndmin=2)\n",
    "\n",
    "    # Create a counter which is used for limiting the number of iterations used if maxiterations has been defined\n",
    "    k = 0\n",
    "    \n",
    "    # Begin main search loop\n",
    "    while maxiterations is -1 or k < maxiterations:\n",
    "        k+=1\n",
    "        c.append(J(theta_in,X_in,Y_in,cost))\n",
    "        \n",
    "        temp = np.copy(theta)\n",
    "        for j in range(0,n):\n",
    "            # making calculations for jth parameter \n",
    "            s = 0 \n",
    "            for i in range(0,m):\n",
    "                s+=  (h(temp.T,np.ravel(X_in[i,:])) - Y_in[i])  * X_in[i,j]\n",
    "            if j == 0:\n",
    "                theta[j] = temp[j] - a_in * (1/m) * s \n",
    "            else:\n",
    "                theta[j] = temp[j]*(1 - a_in * (lambda_in / m)) - a_in * (1/m) * s \n",
    "        thetas = np.vstack((thetas,theta))\n",
    "        \n",
    "        progressing = False\n",
    "        for j in range(0,n):\n",
    "            if(abs(temp[j] - theta[j]) > min_delta_in):\n",
    "                progressing = progressing | True\n",
    "        if not progressing: break \n",
    "    return thetas, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation: From Linear to Non-Linear Model Learning\n",
    "Both Logistic Regression and Linear Regression are essentially linear models. That is the hypotheses that they create are based on linear combinations of inputs. When we know that our model is more complicated than a linear model we can cheat in Linear and Logistic Regression by introducing extra non-linear features which can be fed into the linear classifier. This is exactly what we did when we introduced higher order features in our datasets in examples for Linear Regression and Logistic Regression. \n",
    "\n",
    "The problem with this approach is that it does not scale. In order to achieve complex non-linear decision boundaries in a classification problem we might have to add many additional features. If we are introducing quadratic or other higher order polynomial variants this can lead to an exponential number of features. \n",
    "\n",
    "Neural Networks allow us to build non-linear models without having to manual encode all of these additional higher order features from our base features. The network itself looks at building these higher-order features as part of its own training process. The advantage of this method is that we don't have to guess what higher order features are useful - the neural network will only create higher order features that are shown to have an effect. \n",
    "\n",
    "The disadvantage to this neural network approach is computational cost. Neural Networks are orders of magnitude more expensive to train than a logistic regression classifier - even if that classifier has had many extra manually created features added through data expansion. From the mid 1990s to the early 2010s this high computational cost meant that neural networks were not practical problem solvers. However faster CPUs and speed-ups made possible by GPUs have allowed Neural Networks to finally realize their promise. \n",
    "\n",
    "## Logistic Units\n",
    "Neural Networks are architectures of simple processing units called neurons which are inspired by neurons in the brain but which are really just simple computational units which calculate a function of their input. Taken individually these neurons cannot achieve much, but when integrated into a complex architecture and trained with a powerful learning algorithm they can build complex non-linear models over our data. \n",
    "\n",
    "In practice many different types of neurons can be used to implement a neural network. Here we will begin by focusing on one of the most important of the these, the logistic unit or logistic neuron. \n",
    "\n",
    "A logistic unit is based on the logistic classifier as reviewed last week. In simple terms, the logistic unit makes a binary classification decision based on the application of the logistic function to the negative scalar product of an input vector times some parameters. This is formalized in our familiar function below: \n",
    "\n",
    "\\begin{equation}\n",
    "h_{\\theta}(x) = \\frac{1}{1 + x^{-\\theta x}}\n",
    "\\end{equation}\n",
    "\n",
    "or\n",
    "\n",
    "\\begin{equation}\n",
    "h_{\\theta}(x) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation}\n",
    "z = \\theta x \n",
    "\\end{equation}\n",
    "\n",
    "In the above $\\theta$ are our model parameters, $x$ is our input data vector, and $h$ is the output of our function which is commonly referred to as the hypothesis. $z$ is the linear product of our parameters and input values and is often referred to as the **logit**. \n",
    "\n",
    "We interpret the logistic function as follows: \n",
    "\n",
    "\\begin{equation}\n",
    "h_{\\theta}(x) = P(Y=1 \\;|\\; x_{i},\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "meaning that we must apply a threshold to $h$ to get our classification decision. \n",
    "\n",
    "As mentioned we can have many different types of neurons. The most significant difference between neuron types is the output or **activation function**. In the above the activation function is the logistic function, but a linear activation function can for example be used to produce a continuous real valued output from a neuron. \n",
    "\n",
    "For example we can define and apply a simple linear classifier over two input variables. In the example below our target $Y$ indicates whether a part for a truck was found to be faulty following a vehicle inspection. Feature $x1$ indicates the number of years since the part was replaced and $x2$ indicates the distance the truck has traveled since the last inspection in kilometers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf8AAAFdCAYAAAANJWRbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY/4/hTWFtH2MfoQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5000150e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the target values\n",
    "Y =  [0,0,0,0,0,1,0,1,1,1,1,1,1]\n",
    "\n",
    "# Define the 2 input features in a instance wise fashion\n",
    "X = np.array([[0.3,50000],\n",
    "              [3.2,100000],\n",
    "              [0.4,170000],\n",
    "              [0.8,120000],\n",
    "              [2,150000],\n",
    "              [1.2,250000],\n",
    "              [1.0,160000],\n",
    "              [2,210000],\n",
    "              [2,300000],\n",
    "              [2,400000],\n",
    "              [3,300000],\n",
    "              [3,340000],\n",
    "              [1,350000]\n",
    "             ])\n",
    "\n",
    "# standardize our input data\n",
    "X, means, stds = standardize(X)\n",
    "\n",
    "# Define the function parameters - these were figured out using gradient descent (not shown here)\n",
    "theta = [3.6,2.6,11]\n",
    "\n",
    "# initialize values for features x1 and x2\n",
    "x1_range = np.arange(0,4,0.1) \n",
    "x2_range = np.arange(0.0,500000,1000.0) \n",
    "\n",
    "x1s, x2s = np.meshgrid(x1_range, x2_range) # remember - plt.surface likes this meshgrid output\n",
    "mesh_rows, mesh_cols = x1s.shape\n",
    "\n",
    "# construct 1D arrays for x1, x2 and x0\n",
    "x1 = np.ravel(x1s)\n",
    "x2 = np.ravel(x2s)\n",
    "x = np.vstack((x1,x2)).T\n",
    "\n",
    "# rescale our data based on our normalization parameters\n",
    "x =  (x-means)/stds\n",
    "\n",
    "# define the new matrix X based around our standardized vector for X previously obtained \n",
    "x_0 = np.array([1]*len(x2))\n",
    "x = np.hstack((x_0[:, np.newaxis],x))\n",
    "\n",
    "# perform calculations\n",
    "z = h_logistic(theta,x.T)\n",
    "\n",
    "# reshape result to have the same organization as result of meshgrid\n",
    "zs = np.reshape(z,(mesh_rows,mesh_cols))\n",
    "x1s = np.reshape(x[:,1],(mesh_rows,mesh_cols))\n",
    "x2s = np.reshape(x[:,2],(mesh_rows,mesh_cols))\n",
    "\n",
    "for l in range(len(Y)):\n",
    "    if Y[l] == 0:\n",
    "        plt.plot(X[l,0],X[l,1],'o',c='r')\n",
    "    elif Y[l] == 1:\n",
    "        plt.plot(X[l,0],X[l,1],'o',c='b')\n",
    "    else: print(Y[l])\n",
    "\n",
    "plt.contour(x1s, x2s, zs, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logistic neuron is simply a different architectural perspective on the logistic function. Structurally an artificial neuron has the following components:\n",
    " * A set of inputs x - these are the values that are fed into the neuron \n",
    " * A set of weights w - each weight w_{i} can be interpreted as the relative strength (positive or negative) that we assign to an individual input value x_{i}\n",
    " * A cell body which computes the output for this neuron - the cell body can compute different types of outputs. For our purposes here we assume the output is a logistic function over the inputs x and the weights w. \n",
    " * An output which holds the computed logistic function\n",
    " \n",
    "We can illustrate this structure with respect to our faulty truck part classifier with the figure below. \n",
    "\n",
    "![Logistic Unit](figures/neurons.png)\n",
    "\n",
    "As previously described, in logistic regression we appended a vector of 1s to our input data array X in order to facilitate a vectorized implementation of linear functions. The same is accomplished in the architecture of the neuron through the addition of the **bias unit** which:\n",
    " 1. is always assumed to have a value = 1\n",
    " 2. Is usually referenced as the $0^{th}$ input unit to the neuron.  \n",
    "\n",
    "In the example above we draw in the bias unit and weights explicitly, but it is important to note that in drawings we usually omit the bias unit and often the values for weights. Thus the example above would more normally be drawn as shown below. \n",
    "\n",
    "![Logistic Unit](figures/neurons2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Logistic Units\n",
    "Logistic units can be combined to implement more complex functions. We can examine this by looking at the design of individual binary operations, and then the combination of these units to achieve more sophisticated operations. \n",
    "\n",
    "### AND Units\n",
    "\n",
    "Consider first the construction of the simple AND operator defined over two variables x1 and x2. The desired logic for an AND operation is defined as follows:\n",
    "\n",
    "x1 | x2 | x1 AND x2\n",
    "--- | --- | :---:\n",
    "0 |\t0 |\t0\n",
    "0 |\t1 |\t0\n",
    "1 |\t0 |\t0\n",
    "1 | 1 | 1\n",
    "\n",
    "where x1, x2 and are result are all binary values. \n",
    "\n",
    "It is relatively easy to find the parameters for a logistic unit which will result in computing an AND function. Such a unit (illustrated fully with bias units and weights) is illustrated below. \n",
    "\n",
    "![AND Unit](figures/and.png)\n",
    "\n",
    "Noting that the value for $x0 = 1$ and $x1,x2 \\in \\{0,1\\}$, we see that the parameter on the bias unit pushes the scalar product of inputs and parameters to a minimum value of -3. The only way this scalar sum of inputs and weights can go above 1 is if both the inputs x1 and x2 are 1. This is summarized in the table below:\n",
    "\n",
    "x1 | x2 | $\\theta x$ | g($\\theta$ x)\n",
    "--- | --- | :---: | :---:\n",
    "0 |\t0 |\t-3 | 0\n",
    "0 |\t1 |\t-1 | 0 \n",
    "1 |\t0 |\t-1 | 0 \n",
    "1 | 1 | 1  | 1 \n",
    "\n",
    "### OR Units\n",
    "\n",
    "Similarly the OR operator can be defined over two binary variables $x1$ and $x2$. The logic for the OR operator is summarized below:\n",
    "\n",
    "x1 | x2 | x1 OR x2\n",
    "--- | --- | :---:\n",
    "0 |\t0 |\t0\n",
    "0 |\t1 |\t1\n",
    "1 |\t0 |\t1\n",
    "1 | 1 | 1\n",
    "\n",
    "By choosing appropriate weights for a logistic unit we can once again achieve the desired logic as illustrated below. \n",
    "\n",
    "![OR Unit](figures/or.png)\n",
    "\n",
    "Here the bias unit only partly pushes the scalar product of inputs and parameters below 1. In the case of either or both x1 and x2 being set to 1, the total scalar product will be greater than 0 with the result being that the logistic function has a total output 1. This can be illustrated in the table below. \n",
    "\n",
    "x1 | x2 | $\\theta x$ | g($\\theta$ x)\n",
    "--- | --- | :---: | :---:\n",
    "0 |\t0 |\t-1 | 0\n",
    "0 |\t1 |\t1  | 0 \n",
    "1 |\t0 |\t1  | 0 \n",
    "1 | 1 | 1  | 1 \n",
    "\n",
    "### NOT Units\n",
    "\n",
    "The final atomic unit we will consider is the NOT unit. The NOT unit takes only one explicit input $x1$ and returns the negation of the value on that input as summarized below:\n",
    "\n",
    "x1 | NOT x1\n",
    "--- | :---:\n",
    "0 |\t1 \n",
    "1 |\t0 \n",
    "\n",
    "A logistic unit which simulates the behaviour of a NOT operation when applied to binary values is illustrated below. \n",
    "\n",
    "![OR Unit](figures/not.png)\n",
    "\n",
    "Here when x1 is set the 0, the effect of the bias unit with its weight will be to force the scalar product $\\theta x$ to be greater than 0. However if $x1=1$ the strong negative value of the weight on $x2$ will lead the scalar product $\\theta x$ to be less than 0. This is summarized in the table below. \n",
    "\n",
    "x1  | $\\theta x$ | g($\\theta$ x)\n",
    "--- | :---:      | :---:\n",
    "0   |     1      |        1\n",
    "1   |    -1      |        0 \n",
    "\n",
    "### XNOR Units\n",
    "\n",
    "Given the AND, OR, and NOT units defined above we can implement more sophisticated functtions based on those units. To illustrate let us consider the XNOR operation which is defined with the following truth table:\n",
    "\n",
    "x1 | x2 | x1 XNOR x2\n",
    "--- | --- | :---:\n",
    "0 |\t0 |\t1\n",
    "0 |\t1 |\t0\n",
    "1 |\t0 |\t0\n",
    "1 | 1 | 1\n",
    "\n",
    "XNOR requires both values $x1$ and $x2$ to have the same inputs. The XNOR operation may be defined in terms of our 3 fundamental operations as follows: \n",
    "\n",
    "\\begin{equation}\n",
    "x1\\; XNOR\\; x2 = (x1\\; AND\\; x2)\\; OR\\; ((NOT\\; x1)\\; AND\\; (NOT\\; x2))\n",
    "\\end{equation}\n",
    "\n",
    "By allowing the outputs of some units to act as inputs to another units we can implement the above equation with logistic units as follows: \n",
    "\n",
    "![XNOR Unit](figures/xnor.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Architecture of a Neural Network\n",
    "\n",
    "The network of logistic units used above to define the XNOR operation captures many of the principles of standard neural networks. In this section we discuss the standard basic form of a Neural Network, which is the Feedforward Network. We discuss the Feedforward Network specifically where the network is built exclusively out of logistic units. Networks built out of many other unit types are possible and we will address them later. \n",
    "\n",
    "### Layers of the Network \n",
    "\n",
    "The Feedforward Neural Network consists of at least three layers of artificial neurons as illustrated below. \n",
    "\n",
    "![Feed Forward Neural Network](figures/ffnn.png)\n",
    "\n",
    "In the Feedforward architecture we usually talk about three distinct layer types, i.e., the input layer, the hidden layers, and the output layer. During processing an input layer is clamped to an input data vector, i.e., $x$. The output layer meanwhile captures the result of our complete model, i.e., $Y$. One or more hidden layers can be found between the input and output layers. The hidden layers encode higher order features which operate over the input data which transform the data prior to the final classification at the output layer. \n",
    "\n",
    "Each node in layer $j$ is connected to each node in layer $j-1$ through connections which we refer to as weights. The exception to this is layer 0 which is directly clamped to input data as noted above. In practice weights are simply the parameters we assumed in linear and logistic regression. \n",
    "\n",
    "### Multi-class Classification\n",
    "\n",
    "In the example above we have only one neuron in the output layer but there can be many more. For example in the larger network below thee are three output units. \n",
    "\n",
    "![Feed Forward Neural Network](figures/ffnn2.png)\n",
    "\n",
    "This means that we can build 3 binary classifiers in parallel. This gives us our normal way of handling multi-class classification problems. For a target, e.g., animal type, with 3 classes, e.g., dog, cat and hamster, we create a binary output unit for each of those 3 types. In training the network on a multi-class classification problem we expect only one output unit to be active at a time. During classification we ideally want only one output unit to be active, but in practice most will have some activity and we select the output node that has the highest activity as our hypothesised class. This is referred to as the **One versus All** method. \n",
    "\n",
    "### Bias Units\n",
    "\n",
    "Each layer (with the exception of the output layer) also has a bias unit associated with the layer. We include the bias unit in the network below for illustration, and note that bias unit in layer $j$ is not connected to units in unit $j-1$. The is because the value of the bias unit is fixed to 1. \n",
    "\n",
    "![Feed Forward Neural Network with Bias Units](figures/ffnn3.png)\n",
    "\n",
    "### Deep Networks \n",
    "\n",
    "Deep Feedforward Neural Networks are simply defined as Feedforward Netowrks that have more than one hidden layer. For example even this simple model below would be technically referred to as a Deep Network: \n",
    "\n",
    "![Deep Feed Forward Neural Network](figures/dnn.png)\n",
    "\n",
    "Although more typically our networks will in practice have many more nodes at each layer and a number of hidden layers. \n",
    "\n",
    "![Deep Feed Forward Neural Network](figures/dnn2.png)\n",
    "\n",
    "## Notational Conventions\n",
    "\n",
    "When working through descriptions of neural network one of the most challenging problems is keeping track of the notation - particularly with respect to indexing through multiple layers of the network and multiple training and testing examples. \n",
    "\n",
    "### Notation for Data\n",
    "As with any other supervised machine learning method, our model is trained with a set of m input-output tuples:\n",
    "\n",
    "\\begin{equation}\n",
    "Data_{Training} = \\{ (x^{0},y^{0}),(x^{1},y^{1}),(x^{2},y^{2})\\dots (x^{m-1},y^{m-1}) \\}\n",
    "\\end{equation}\n",
    "\n",
    "Each tuple is an individual training case which we index with a superscript. \n",
    "\n",
    "For each individual training case we assume vectorized inputs and outputs. The length of an input vector must be equal to the number of units on the input layer (not counting the bias unit). Similarly the length of an output vector must be equal to the number of units on the output layer of the network. Thus for a network with 3 input units and 2 output units:\n",
    "\n",
    " * $x^{i} \\in \\{0,1\\}^{3\\times1}$\n",
    " * $y^{i} \\in \\{0,1\\}^{2\\times1}$\n",
    " \n",
    "### Notation for Layers\n",
    "A neural network is built out of a number of layers. A layer typically includes both the nodes and a set of associated weights. In indexing and describing the layers of an individual network we use the following notation:\n",
    " * $L$ - the total number of layers in the network\n",
    " * $s_{l}$ - the number of nodes the the $l^{th}$ layer of the network - excluding bias units. \n",
    "\n",
    "In many sources our first layer is indexed as layer 1 rather than layer 0. However this only makes the programming of these networks more cumbersome. Here we will stick with standard computer science convention of indexing from 0. Hence the input layer is layer 0 and our output layer is layer $L-1$. \n",
    "\n",
    "#### Notation for Nodes\n",
    "\n",
    "Each layer includes a set of activation units. We rarely index the units themselves. Instead we index the values produced at those units. We refer to the output of a node as its activation value. For the first layer the activation value for a given unit is simply the corresponding input value from the data. For all other layers the activation value for a unit is its computed output. For logistic units, this is the logistic function applied to the scalar product of the values of the weights and activations of the $j-1$th layer. \n",
    "\n",
    "We denote activation values with the character $a$. A superscript on $a$ is used to index layers. A subscript is used to index the activation values for an individual unit within that layer. To illustrate:\n",
    " * $a^{0}$ refers to the vector of activation values for our input layer. This will correspond to our input vector $x$\n",
    " * $a^{L-1}$ refers to the vector of activation values for our output layer. This will be our output vector $y$. \n",
    " * $a^{1}$ refers to the vector of activation values on our (first) hidden layer. \n",
    " * $a^{1}_{0}$ refers to the activation value of the bias unit on our (first) hidden layer. This will always be 1. \n",
    " * $a^{1}_{3}$ refers to the activation value of the 3rd (non-bias) unit in the (first) hidden layer as illustrated below. \n",
    " \n",
    "![Indexing Units](figures/activation.png)\n",
    "\n",
    "As well as indexing the activation values of the layer, it is often useful to index the partial result $\\theta X$. We refer to this value as $z$ and use the same indexing conventions as with the activation values. It is worth noting though that no values for $z$ exist for the input layer as our activation values for that layer come directly from the input vector $x$ rather than through an application of the logistic function. \n",
    " \n",
    "#### Notation for Weights\n",
    "\n",
    "The weights associated with layer $j$ are the weights which follow the outputs from the nodes rather than the weights which feed into nodes. Therefore all layers in the network apart from the output layer have a set of weights. \n",
    "\n",
    "Weights are denoted with the character $\\theta$. A superscript on $\\theta$ is used to index layers. A subscript is used to index individual weights within a layer of weights. That subscript is based on a tuple where the first index refers to the source of the weight and the second index refers to the target. \n",
    "\n",
    " * $\\theta^{0}$ refers to the weights emerging from the input layer\n",
    " * $\\theta^{L-2}$ refers to the layer of weights feeding in to the output layer\n",
    " * $\\theta^{1}$ refers to the weights emerging from the (first) hidden layer\n",
    " * $\\theta^{1}_{0,1}$ refers to the weight from the bias unit in the (first) hidden layer to the first unit in the following layer\n",
    " * $\\theta^{1}_{5,2}$ refers to the weight highlighted in the figure below. \n",
    " \n",
    "![Indexing Weights](figures/weights.png)\n",
    "\n",
    "Note that the target index should never be equal to 0 since bias units are never updated. Note also that the output layer does not have a set of weights defined, therefore $\\theta^{L-1}$ and $\\theta^{L}$ are both invalid indices.  \n",
    "\n",
    "## The Feedforward Algorithm\n",
    "\n",
    "The feedfoward algorithm is a parallelized (within a layer) and sequentialized (down through the layers) extension of applying individual activation functions. In our case below the activation functions are all logistic functions but it is important to remember that other possibilities exist. \n",
    "\n",
    "To illustrate the use of the feedforward algorithm let us assume a network with the topology illustrated below. \n",
    "\n",
    "![Indexing Weights](figures/example.png)\n",
    "\n",
    "Here:\n",
    " * L = 4\n",
    " * $S_{0} = 3$, $S_{1} = 3$, $S_{2} = 3$, $S_{3} = 1$\n",
    "\n",
    "We also assume a single input X. Since $S_{0} = 3$ then the length of the individual input vector is also 3. We also assume a randomly initialized set of weights. \n",
    "\n",
    "The feedforward algorithm itself can be implemented simply as a sequence of logistic function executions as illustrated in the example below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.86108579]]\n"
     ]
    }
   ],
   "source": [
    "# Define the input vector to our network\n",
    "X = np.array([[0.5,1.1,1.6]])\n",
    "X = X.T\n",
    "\n",
    "# Define the weights for all layers\n",
    "Theta1 = np.random.rand(4,3)\n",
    "Theta2 = np.random.rand(4,3)\n",
    "Theta3 = np.random.rand(4,1)\n",
    "\n",
    "# construct the activation values for layer 1 from X and a bias value\n",
    "a1 = np.vstack(([1.0],X))\n",
    "\n",
    "# calculate z for the first hidden layer\n",
    "z2 = np.matmul(Theta1.T,a1)\n",
    "# calculate a for the first hidden layer\n",
    "a2 = np.vstack(([1.0],1 / (1 + np.exp(-z2))))\n",
    "\n",
    "# calculate z for the second hidden layer\n",
    "z3 = np.matmul(Theta2.T,a2)\n",
    "# calculate a for the second hidden layer\n",
    "a3 = np.vstack(([1.0],1 / (1 + np.exp(-z3))))\n",
    "\n",
    "# calculate z for the output layer\n",
    "z4 = np.matmul(Theta3.T,a3)\n",
    "# calculate a for the output layer\n",
    "a4 = 1 / (1 + np.exp(-z4))\n",
    "\n",
    "print(a4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the following about the implementation above:\n",
    " * When we initialize weights we must remember that weights must also be initialized for weights coming from (but not to) bias units. \n",
    " * When we construct the activation values for a given layer we add the bias unit value to the layer at that point \n",
    " * No bias unit is added to the output layer when we compute the final set of activation functions\n",
    "\n",
    "Given a set of appropriate parameters we can rework the code above with some arrays for storage and a loop over layers which gives us a more extensible implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.86108579]]\n"
     ]
    }
   ],
   "source": [
    "# Define the input vector to our network\n",
    "X = np.array([[0.5,1.1,1.6]])\n",
    "X = X.T\n",
    "\n",
    "# Define the number of (visible) units in each layer \n",
    "S = [3,3,3,1]\n",
    "\n",
    "# Define empty arrays for theta, activation and z values\n",
    "theta = [None] * len(S)\n",
    "a =  [None] * len(S)\n",
    "z =  [None] * len(S)\n",
    "\n",
    "# Define the weights for all layers\n",
    "for idx, val in enumerate(S):\n",
    "    if idx == (len(S)-1): \n",
    "        break\n",
    "    theta[idx] = np.random.rand( S[idx]+1,S[idx+1])\n",
    "\n",
    "# Implement the Feedforward Algorithm \n",
    "for idx, val in enumerate(S):\n",
    "    if idx == 0:\n",
    "        # construct the activation values for layer 1 from X and a bias value\n",
    "        a[idx] = np.vstack(([1.0],X))\n",
    "    elif idx == (len(S)-1):\n",
    "        # calculate z for the output layer\n",
    "        z[idx] = np.matmul(theta[idx-1].T,a[idx-1])\n",
    "        # calculate a for the output layer\n",
    "        a[idx] = 1 / (1 + np.exp(-z[idx]))\n",
    "    else:\n",
    "        # calculate z for a hidden layer\n",
    "        x = np.matmul(theta[idx-1].T,a[idx-1])\n",
    "        z[idx] = x\n",
    "        # calculate a for a hidden layer\n",
    "        a[idx] = np.vstack(([1.0],1 / (1 + np.exp(-z[idx]))))\n",
    "print(a4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Representation the Easy Way\n",
    "We will be using TensorFlow for Neural Network examples. TensorFlow is a powerful engine for Tensor algebra and training of neural networks. \n",
    "\n",
    "A Tensor is a mathematical construct that can be thought of a multi-dimensional array with some extra mathematical properties. For our own purposes we do not need to worry about these extra mathematical properties and we can instead just think of a Tensor as being a pseudonym for vectors, matrices and generally multi-dimensional arrays. \n",
    "\n",
    "Part of TensorFlow's power comes from allowing you to design networks at a very abstract level of execution and have them implemented in low-level C code efficiently. This power however comes at the cost of an initially complex approach to designing the layers of our network and executing any calculations with those networks. \n",
    "\n",
    "The Computational Graph is the execution workflow that TensorFlow executes for us. When defining a network we are defining a Computational Graph to be executed later. \n",
    "\n",
    "TensorFlow has two important forms of variables. A TensorFlow Variable is in many ways similar to normal variables in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"x:0\", shape=(5,), dtype=float32)\n",
      "Tensor(\"y/read:0\", shape=(5,), dtype=float32)\n",
      "[  1.   4.   9.  16.  25.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant([1.,2.,3.,4.,5.], name='x')\n",
    "y = tf.Variable(x ** 2, name='y')\n",
    "\n",
    "model = tf.global_variables_initializer()\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(model)\n",
    "    print(session.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TensorFlow placeholder is a simple form of variable which we will later populate with data. We feed data into our Network using these placeholders. To illustrate consider the example below where a TensorFlow placeholder is first created to represent a length 5 vector of input data that is expected. A TensorFlow power operator is then created based on the TensorFlow placeholder. The calculation is then run in the TensorFlow session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(5,), dtype=float32)\n",
      "Tensor(\"pow_1:0\", shape=(5,), dtype=float32)\n",
      "[  1.   4.   9.  16.  25.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(\"float\", 5)\n",
    "y = x ** 2\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    result = session.run(y, feed_dict={x: [1, 2, 3, 4, 5]})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often use Placeholders for input and output vectors while using Variables for the internal constructors of the network. \n",
    "\n",
    "To illustrate the application of the complete construction method consider the following adapted from the Backpropogation algorithm given at:\n",
    "https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/multilayer_perceptron.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.88390577]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the number of (visible) units in each layer \n",
    "S = [3,3,3,1]\n",
    "\n",
    "x = tf.placeholder(\"float\", shape=[None, S[0]])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, S[3]])\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([S[0], S[1]])),\n",
    "    'h2': tf.Variable(tf.random_normal([S[1], S[2]])),\n",
    "    'out': tf.Variable(tf.random_normal([S[2], S[3]]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([S[1]])),\n",
    "    'b2': tf.Variable(tf.random_normal([S[2]])),\n",
    "    'out': tf.Variable(tf.random_normal([S[3]]))\n",
    "}\n",
    "\n",
    "def ff_network(x, weights, biases):\n",
    "    # Hidden layer with sigmoid activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.sigmoid(layer_1)\n",
    "    # Hidden layer with sigmoid activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.sigmoid(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "pred = ff_network(x, weights, biases)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    result = sess.run(pred, feed_dict={x:[[0.5,1.1,1.6]]})\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
